set.seed(1)
#### Principal Components Regression
pcr.fit = pcr(lpsa~.,
data = trainst,
scale = F,
validation = "CV",
## Segments for 5-fold randomised cross-validation:
segments = 5)
## Find the best number of components,
## regenerating part of Figure 3.7 on page 62
summary(pcr.fit)
validationplot(pcr.fit,
cex = 1,
main = "Principal Components Regression",
val.type="MSEP",
xlab = "number of components",
ylab = "cv error" )
itemp = which.min(pcr.fit$validation$PRESS)     # 8
itemp.mean = pcr.fit$validation$PRESS[itemp]/67
mean((pcr.fit$validation$pred[,,itemp]- trainst[,9])^2)
itemp.sd = sd((pcr.fit$validation$pred[,,itemp]-
trainst[,9])^2)/sqrt(67)
abline(h= itemp.mean, lty=2, col="purple")
k.pcr = min((1:pcr.fit$validation$ncomp)[pcr.fit$validation$PRESS/67 < itemp.mean + itemp.sd])  # the chosen
abline(v = k.pcr, lty=2, col="blue")
# REG_CHOICE: 1 (does principal component regression),
set.seed(16)
#### Principal Components Regression
pcr.fit = pcr(lpsa~.,
data = trainst,
scale = F,
validation = "CV",
## Segments for 5-fold randomised cross-validation:
segments = 5)
## Find the best number of components,
## regenerating part of Figure 3.7 on page 62
summary(pcr.fit)
validationplot(pcr.fit,
cex = 1,
main = "Principal Components Regression",
val.type="MSEP",
xlab = "number of components",
ylab = "cv error" )
itemp = which.min(pcr.fit$validation$PRESS)     # 8
itemp.mean = pcr.fit$validation$PRESS[itemp]/67
mean((pcr.fit$validation$pred[,,itemp]- trainst[,9])^2)
itemp.sd = sd((pcr.fit$validation$pred[,,itemp]-
trainst[,9])^2)/sqrt(67)
abline(h= itemp.mean, lty=2, col="purple")
k.pcr = min((1:pcr.fit$validation$ncomp)[pcr.fit$validation$PRESS/67 < itemp.mean + itemp.sd])  # the chosen
abline(v = k.pcr, lty=2, col="blue")
### Coefficients summary with intercept
B <- coef(pcr.fit, ncomp = k.pcr, intercept = TRUE)
B
# estimating mean prediction error
test.pcr = predict(pcr.fit, as.matrix(testst[,1:8]),
ncomp = k.pcr)
# mean (absolute) prediction error
te5 = mean(abs(testst[,9]- test.pcr))
te5
# mean (squared) prediction error
mean((testst[,9]-test.pcr)^2)
# standard error of mean (squared) prediction error
se5 = sd((testst[,9]-test.pcr)^2)/sqrt(30)
se5
# REG_CHOICE: 1 (does principal component regression),
set.seed(18)
#### Principal Components Regression
pcr.fit = pcr(lpsa~.,
data = trainst,
scale = F,
validation = "CV",
## Segments for 5-fold randomised cross-validation:
segments = 5)
## Find the best number of components,
## regenerating part of Figure 3.7 on page 62
summary(pcr.fit)
validationplot(pcr.fit,
cex = 1,
main = "Principal Components Regression",
val.type="MSEP",
xlab = "number of components",
ylab = "cv error" )
itemp = which.min(pcr.fit$validation$PRESS)     # 8
itemp.mean = pcr.fit$validation$PRESS[itemp]/67
mean((pcr.fit$validation$pred[,,itemp]- trainst[,9])^2)
itemp.sd = sd((pcr.fit$validation$pred[,,itemp]-
trainst[,9])^2)/sqrt(67)
abline(h= itemp.mean, lty=2, col="purple")
k.pcr = min((1:pcr.fit$validation$ncomp)[pcr.fit$validation$PRESS/67 < itemp.mean + itemp.sd])  # the chosen
abline(v = k.pcr, lty=2, col="blue")
### Coefficients summary with intercept
B <- coef(pcr.fit, ncomp = k.pcr, intercept = TRUE)
B
# estimating mean prediction error
test.pcr = predict(pcr.fit, as.matrix(testst[,1:8]),
ncomp = k.pcr)
# mean (absolute) prediction error
te5 = mean(abs(testst[,9]- test.pcr))
te5
# mean (squared) prediction error
mean((testst[,9]-test.pcr)^2)
# standard error of mean (squared) prediction error
se5 = sd((testst[,9]-test.pcr)^2)/sqrt(30)
se5
# REG_CHOICE: 1 (does principal component regression),
set.seed(8)
#### Principal Components Regression
pcr.fit = pcr(lpsa~.,
data = trainst,
scale = F,
validation = "CV",
## Segments for 5-fold randomised cross-validation:
segments = 5)
## Find the best number of components,
## regenerating part of Figure 3.7 on page 62
summary(pcr.fit)
validationplot(pcr.fit,
cex = 1,
main = "Principal Components Regression",
val.type="MSEP",
xlab = "number of components",
ylab = "cv error" )
itemp = which.min(pcr.fit$validation$PRESS)     # 8
itemp.mean = pcr.fit$validation$PRESS[itemp]/67
mean((pcr.fit$validation$pred[,,itemp]- trainst[,9])^2)
itemp.sd = sd((pcr.fit$validation$pred[,,itemp]-
trainst[,9])^2)/sqrt(67)
abline(h= itemp.mean, lty=2, col="purple")
k.pcr = min((1:pcr.fit$validation$ncomp)[pcr.fit$validation$PRESS/67 < itemp.mean + itemp.sd])  # the chosen
abline(v = k.pcr, lty=2, col="blue")
### Coefficients summary with intercept
B <- coef(pcr.fit, ncomp = k.pcr, intercept = TRUE)
B
# estimating mean prediction error
test.pcr = predict(pcr.fit, as.matrix(testst[,1:8]),
ncomp = k.pcr)
# mean (absolute) prediction error
te5 = mean(abs(testst[,9]- test.pcr))
te5
# mean (squared) prediction error
mean((testst[,9]-test.pcr)^2)
# standard error of mean (squared) prediction error
se5 = sd((testst[,9]-test.pcr)^2)/sqrt(30)
se5
te1
coef(res.bestcv$BestModel)
## note that they are not the same as Table 3.3 on page 63 due to the chosen labmda and fitting algorithm
# Intercept
fitlasso$a0
# beta
fitlasso$beta
cv.out$lambda.1se
# Duplicate part of Figure 3.7 (the lasso regularization results)
## use glmnet in package glmnet
library(glmnet)
# use 5-fold cross-validation to choose best lambda
set.seed(18)
cv.out = cv.glmnet(x = as.matrix(trainst[,1:8]),
y= as.numeric(trainst[,9]),
nfolds = 5,
alpha = 1, # alpha = 1 => lasso
standardize = F) ## has standardized
# the best lambda chosen by 5-fold cross-validation
lambda.5fold = cv.out$lambda.1se
# get the optimal value of lambda:
# apply Lasso with chosen lambda
fitlasso = glmnet(x =  as.matrix(trainst[,1:8]),
y=   as.numeric(trainst[,9]),
alpha = 1,
lambda = lambda.5fold,
standardize= F,
thresh = 1e-12)
lambda.5fold
## note that they are not the same as Table 3.3 on page 63 due to the chosen labmda and fitting algorithm
# Intercept
fitlasso$a0
# beta
fitlasso$beta
# estimating mean prediction error
test.lasso = predict(fitlasso,
newx=as.matrix(testst[,1:8]))
# mean (absolute) prediction error
mean(abs(testdata[,9]-test.lasso))
# mean (squared) prediction error
mean((testdata[,9]-test.lasso)^2)
# standard error of mean (squared) prediction error
sd((testdata[,9]-test.lasso)^2)/sqrt(30)
sd((testdata[,9]-test.lasso)^2)/sqrt(30)
test.lasso
mean(abs(testdata[,9]-test.lasso))
set.seed(221)
## == LASSO == ##
# Fit lasso model on training data
lassobic_fit <- glmnet(
x = as.matrix(trainst[,1:8]),
y = as.numeric(trainst[,9]),
lambda = seq(0, 1, length.out = 1000))
bic <- (- lassobic_fit$nulldev +
deviance(lassobic_fit) +
log(lassobic_fit$nobs)* lassobic_fit$df)/lassobic_fit$nobs
## Draw plot of coefficients
plot(seq(0,1,length.out = 1000), bic, type="l",
col = "blue",
main = "The LASSO Model with BIC",
lwd= 2.3,
xlab = "Lambda", ylab = "BIC")
abline(h = min(bic),
lty = 2,
family="Symbol",
col = "purple")
abline(v = seq(0,1,length.out = 1000)[min(which(bic==min(bic)))],
lty =2, col = "purple")
### Best lambda chosen
bestlam = seq(0,1,length.out = 1000)[min(which(bic == min(bic)))]
bestlam
### Coefficients Estimates
bic_est = coef(glmnet(as.matrix(trainst[,1:8]), trainst[,9],
lambda = bestlam))
bic_est
##### Now we can calculate the forecast: Test error
test_error4 = sum((model.matrix(~., (testst[,1:8])) %*%
coef(glmnet(as.matrix(trainst[,1:8]), trainst[,9],
lambda = bestlam)) - testst[,9])^2)/30
test_error4
# standard error of mean (squared) prediction error
sd4 = sd((model.matrix(~., (testst[,1:8])) %*%
coef(glmnet(as.matrix(trainst[,1:8]), trainst[,9],
lambda = bestlam)) - testst[,9])^2)/sqrt(30)
sd4
lassobic_fit$beta
bic_est
plots.dir.path <- list.files(tempdir(), pattern="rs-graphics", full.names = TRUE);
plots.png.paths <- list.files(plots.dir.path, pattern=".png", full.names = TRUE)
file.copy(from=plots.png.paths, to="path_to_your_dir")
# Chunk 1
library(tidyverse)
library(leaps)
library(lattice)
library(caret)
library(glmnet)
library(pls)
require(ggplot2)
# Chunk 2
df <- read.delim("./prostate.data")
modeldf0 = df[2:11] ## Include indicators
modeldf = df[2:10]
## Overview of the data covariates in model
p = dim(modeldf)[2]-1
# the last column is response
p
# Obs in data
totalSamples = dim(modeldf)[1]
totalSamples
# Chunk 3
## sampling segments
library(bestglm)
k = 5
set.seed(1818)
folds = sample(1:k, nrow(df), replace = TRUE)
## Set up the train and test data
testdata = modeldf0 %>%
filter(train == FALSE) %>%
select(-train)
traindata = modeldf0 %>%
filter(train == TRUE) %>%
select(-train)
# Chunk 4
# standardization of predictors
prostate = df %>%
dplyr::select(-X)
trainst <- traindata
for(i in 1:8) {
trainst[,i] <- trainst[,i] - mean(prostate[,i]);
trainst[,i] <- trainst[,i]/sd(prostate[,i]);
}
testst <- testdata
for(i in 1:8) {
testst[,i] <- testst[,i] - mean(prostate[,i]);
testst[,i] <- testst[,i]/sd(prostate[,i]);
}
# Chunk 5
set.seed(21)
## Fit model: with k = 5
res.bestcv = bestglm(trainst,
IC= "CV",
CVArgs = list(Method="HTF", K=5, REP=1))
## Extract the cv.error and sd. for cv.error
res.bestcv$Subsets$CV
res.bestcv$Subsets$sdCV
# Chunk 6
## Show top model
res.bestcv
coef(res.bestcv$BestModel)
# Chunk 7
v = c(0:8)
error1 = res.bestcv$Subsets$CV
sd1 = res.bestcv$Subsets$sdCV
fit1 = data.frame(cbind(v, error1, sd1))
ggplot(fit1, aes(x = c(0:8),
y = error1
)) +
geom_line(aes(col = '#E69F00')) +
geom_point(aes(col = '#999999'))+
geom_errorbar(aes(ymin = error1 - sd1,
ymax = error1 + sd1),
width = 0.2,
position = position_dodge(0.05)) +
labs(title="Best Subset Method with 5-fold CV",
x="# of Variables", y = "CV Error")+
theme_classic() +
theme(legend.position="none") +
geom_hline(yintercept= min(error1),
linetype="dashed", color = "purple") +
geom_vline(xintercept= 2,
linetype="dashed", color = "purple")
# Chunk 8
## Yhat: predicted value on the testdata
yh <- as.matrix(predict
(res.bestcv$BestModel,newx = as.matrix(testst[,1:8]) ))
### test error
te1 = sum(lm(lpsa~lcavol + lweight, testst)$residuals^2)/30
te1
## estimating Std Error
sd1 = sum(sqrt(mean(((yh[1:30] - testst[,9])^2))))/nrow(testst)
sd1
# Chunk 9
set.seed(1)
reg.bestbic = bestglm(trainst,
IC="BIC")
## Model summary coefficients
summary(reg.bestbic)
# Chunk 10
reg.bestbic$BestModel$coefficients
# Chunk 11
error2 = reg.bestbic$Subsets$BIC
fit2 = data.frame(cbind(v, error2))
ggplot(fit2, aes(x = c(0:8),
y = error2
)) +
geom_line(aes( col = '#E69F00')) +
geom_point(aes(col = '#999999')) +
labs(title="Best Subset Method with BIC selection",
x="# of Variables", y = "BIC")+
theme_classic() +
theme(legend.position="none") +
geom_hline(yintercept= min(error2),
linetype="dashed", color = "purple") +
geom_vline(xintercept= 2,
linetype="dashed", color = "purple")
### BIC:
BIC(lm(lpsa~lcavol+lweight, testst))
### test error
## Yhat: predicted value on the testdata
yh2 <- as.matrix(predict
(reg.bestbic$BestModel,newx = as.matrix(testst[,1:8]) ))
### test error
te2 = sum(lm(lpsa~lcavol + lweight, testst)$residuals^2)/30
te2
## estimating Std Error
sd2 = sum(sqrt(mean(((yh2[1:30] - testst[,9])^2))))/nrow(testst)
sd2
# Chunk 12
# Duplicate part of Figure 3.7 (the lasso regularization results)
## use glmnet in package glmnet
library(glmnet)
# use 5-fold cross-validation to choose best lambda
set.seed(18)
cv.out = cv.glmnet(x = as.matrix(trainst[,1:8]),
y= as.numeric(trainst[,9]),
nfolds = 5,
alpha = 1, # alpha = 1 => lasso
standardize = F) ## has standardized
# the best lambda chosen by 5-fold cross-validation
lambda.5fold = cv.out$lambda.1se
# Chunk 13
# get the optimal value of lambda:
# apply Lasso with chosen lambda
fitlasso = glmnet(x =  as.matrix(trainst[,1:8]),
y=   as.numeric(trainst[,9]),
alpha = 1,
lambda = lambda.5fold,
standardize= F,
thresh = 1e-12)
# Chunk 14
## note that they are not the same as Table 3.3 on page 63 due to the chosen labmda and fitting algorithm
# Intercept
fitlasso$a0
# beta
fitlasso$beta
# Chunk 15
plot(cv.out,
xlab="Shrinkage Factors",
main = "The LASSO Model with 5-fold CV",
ylab="Mean CV Error", 0.01)
abline(h=cv.out$cvup[which.min(cv.out$cvm)])
# Chunk 16
# estimating mean prediction error
test.lasso = predict(fitlasso,
newx=as.matrix(testst[,1:8]))
# mean (absolute) prediction error
mean(abs(testdata[,9]-test.lasso))
# mean (squared) prediction error
mean((testdata[,9]-test.lasso)^2)
# standard error of mean (squared) prediction error
sd((testdata[,9]-test.lasso)^2)/sqrt(30)
# Chunk 18
set.seed(221)
## == LASSO == ##
# Fit lasso model on training data
lassobic_fit <- glmnet(
x = as.matrix(trainst[,1:8]),
y = as.numeric(trainst[,9]),
lambda = seq(0, 1, length.out = 1000))
bic <- (- lassobic_fit$nulldev +
deviance(lassobic_fit) +
log(lassobic_fit$nobs)* lassobic_fit$df)/lassobic_fit$nobs
## Draw plot of coefficients
plot(seq(0,1,length.out = 1000), bic, type="l",
col = "blue",
main = "The LASSO Model with BIC",
lwd= 2.3,
xlab = "Lambda", ylab = "BIC")
abline(h = min(bic),
lty = 2,
family="Symbol",
col = "purple")
abline(v = seq(0,1,length.out = 1000)[min(which(bic==min(bic)))],
lty =2, col = "purple")
# Chunk 19
### Best lambda chosen
bestlam = seq(0,1,length.out = 1000)[min(which(bic == min(bic)))]
bestlam
### Coefficients Estimates
bic_est = coef(glmnet(as.matrix(trainst[,1:8]), trainst[,9],
lambda = bestlam))
bic_est
##### Now we can calculate the forecast: Test error
test_error4 = sum((model.matrix(~., (testst[,1:8])) %*%
coef(glmnet(as.matrix(trainst[,1:8]), trainst[,9],
lambda = bestlam)) - testst[,9])^2)/30
test_error4
# standard error of mean (squared) prediction error
sd4 = sd((model.matrix(~., (testst[,1:8])) %*%
coef(glmnet(as.matrix(trainst[,1:8]), trainst[,9],
lambda = bestlam)) - testst[,9])^2)/sqrt(30)
sd4
# Chunk 20
# REG_CHOICE: 1 (does principal component regression),
set.seed(8)
#### Principal Components Regression
pcr.fit = pcr(lpsa~.,
data = trainst,
scale = F,
validation = "CV",
## Segments for 5-fold randomised cross-validation:
segments = 5)
## Find the best number of components,
## regenerating part of Figure 3.7 on page 62
summary(pcr.fit)
validationplot(pcr.fit,
cex = 1,
main = "Principal Components Regression",
val.type="MSEP",
xlab = "number of components",
ylab = "cv error" )
itemp = which.min(pcr.fit$validation$PRESS)     # 8
itemp.mean = pcr.fit$validation$PRESS[itemp]/67
mean((pcr.fit$validation$pred[,,itemp]- trainst[,9])^2)
itemp.sd = sd((pcr.fit$validation$pred[,,itemp]-
trainst[,9])^2)/sqrt(67)
abline(h= itemp.mean, lty=2, col="purple")
k.pcr = min((1:pcr.fit$validation$ncomp)[pcr.fit$validation$PRESS/67 < itemp.mean + itemp.sd])  # the chosen
abline(v = k.pcr, lty=2, col="blue")
# Chunk 21
### Coefficients summary with intercept
B <- coef(pcr.fit, ncomp = k.pcr, intercept = TRUE)
B
# Chunk 22
# estimating mean prediction error
test.pcr = predict(pcr.fit, as.matrix(testst[,1:8]),
ncomp = k.pcr)
# mean (absolute) prediction error
te5 = mean(abs(testst[,9]- test.pcr))
te5
# mean (squared) prediction error
mean((testst[,9]-test.pcr)^2)
# standard error of mean (squared) prediction error
se5 = sd((testst[,9]-test.pcr)^2)/sqrt(30)
se5
plots.dir.path <- list.files(tempdir(), pattern="rs-graphics", full.names = TRUE)
plots.dir.path
plots.dir.path <- list.files(tempdir(), pattern="rs-graphics", full.names = TRUE)
plots.png.paths <- list.files(plots.dir.path, pattern=".png", full.names = TRUE)
file.copy(from=plots.png.paths, to="path_to_your_dir")
# ph for plot history
ph <- .SavedPlots
for(i in 1:lastplot) {
png('plotname.png')
print(ph[i])
dev.off()
}
plots.dir.path <- list.files(tempdir(), pattern="rs-graphics", full.names = TRUE)
plots.png.paths <- list.files(plots.dir.path, pattern=".png", full.names = TRUE)
# ph for plot history
ph <- .SavedPlots
for(i in 1:12) {
png('plotname.png')
print(ph[i])
dev.off()
}
lambda.5fold
