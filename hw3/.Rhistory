mutate(yc= as.factor(yc))
# Chunk 4
## Using scaled data
set.seed (1)
tune.out = tune.svm(yc~.,
data=train_data,
kernel="linear",
cost= exp(seq(-5, 1, len = 50)))
summary(tune.out)
svmfit1 = tune.out$best.model
summary(svmfit1)
# Chunk 5
ypred1 = predict(svmfit1,test_data) %>%
as.numeric()
test.error1 <- 1- sum(ypred1 == test_data$yc)/nrow(test_data)
test.error1
std.error1 <- sd((ypred1 - test_data$yc)^2)/sqrt(nrow(test_data))
std.error1
# Chunk 1
library(tidyverse)
library(broom)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(gbm)      # boosting technique
library(glmnet)
require(ggplot2)
# Chunk 2
## 250 obs. train data
train_data <- read.delim("./synth.tr", header = T, sep = "")
## 1000 obs. test data
test_data <- read.delim("./synth.te", header = T, sep = "")
# Chunk 3
train_data = train_data %>%
mutate(yc= as.factor(yc))
test_data = test_data %>%
mutate(yc= as.factor(yc))
## Using scaled data
set.seed (1)
tune.out = tune(svm, yc~.,
data=train_data,
kernel="linear",
cost= exp(seq(-5, 1, len = 50)))
summary(tune.out)
svmfit1 = tune.out$best.model
summary(svmfit1)
# Chunk 1
library(tidyverse)
library(broom)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(gbm)      # boosting technique
library(glmnet)
require(ggplot2)
# Chunk 2
## 250 obs. train data
train_data <- read.delim("./synth.tr", header = T, sep = "")
## 1000 obs. test data
test_data <- read.delim("./synth.te", header = T, sep = "")
# Chunk 3
train_data = train_data %>%
mutate(yc= as.factor(yc))
test_data = test_data %>%
mutate(yc= as.factor(yc))
# Chunk 4
## Using scaled data
set.seed (1)
tune.out = tune(svm, yc~.,
data= train_data,
kernel="linear",
cost= exp(seq(-5, 1, len = 50)))
summary(tune.out)
svmfit1 = tune.out$best.model
summary(svmfit1)
# Chunk 5
ypred1 = predict(svmfit1,test_data) %>%
as.numeric()
test.error1 <- 1- sum(ypred1 == test_data$yc)/nrow(test_data)
test.error1
std.error1 <- sd((ypred1 - test_data$yc)^2)/sqrt(nrow(test_data))
std.error1
# Chunk 1
library(tidyverse)
library(broom)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(gbm)      # boosting technique
library(glmnet)
require(ggplot2)
# Chunk 2
## 250 obs. train data
train_data <- read.delim("./synth.tr", header = T, sep = "")
## 1000 obs. test data
test_data <- read.delim("./synth.te", header = T, sep = "")
# Chunk 3
train_data = train_data %>%
mutate(yc= as.factor(yc))
# Chunk 4
## Using scaled data
set.seed (1)
tune.out = tune(svm, yc~.,
data= train_data,
kernel="linear",
cost= exp(seq(-5, 1, len = 50)))
summary(tune.out)
svmfit1 = tune.out$best.model
summary(svmfit1)
ypred1 = predict(svmfit1,test_data) %>%
as.numeric()
test.error1 <- 1- sum(ypred1 == test_data$yc)/nrow(test_data)
test.error1
std.error1 <- sd((ypred1 - test_data$yc)^2)/sqrt(nrow(test_data))
std.error1
# Chunk 1
library(tidyverse)
library(broom)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(gbm)      # boosting technique
library(glmnet)
require(ggplot2)
# Chunk 2
## 250 obs. train data
train_data <- read.delim("./synth.tr", header = T, sep = "")
## 1000 obs. test data
test_data <- read.delim("./synth.te", header = T, sep = "")
# Chunk 3
train_data = train_data %>%
mutate(yc= as.factor(yc))
# Chunk 4
## Using scaled data
set.seed (1)
tune.out = tune(svm, yc~.,
data= train_data,
kernel="linear",
cost= exp(seq(-5, 1, len = 50)))
summary(tune.out)
svmfit1 = tune.out$best.model
summary(svmfit1)
ypred1 = predict(svmfit1,test_data) %>%
as.numeric()
test.error1 <- 1- sum(ypred1 == test_data$yc)/nrow(test_data)
test.error1
std.error1 <- sd((ypred1 - test_data$yc)^2)/sqrt(nrow(test_data))
std.error1
sum(ypred1 == test_data$yc)
# Chunk 1
library(tidyverse)
library(broom)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(gbm)      # boosting technique
library(glmnet)
require(ggplot2)
# Chunk 2
## 250 obs. train data
train_data <- read.delim("./synth.tr", header = T, sep = "")
## 1000 obs. test data
test_data <- read.delim("./synth.te", header = T, sep = "")
# Chunk 3
train_data = train_data %>%
mutate(yc= as.factor(yc))
test_data = test_data %>%
mutate(yc= as.factor(yc))
# Chunk 4
## Using scaled data
set.seed (1)
tune.out = tune(svm, yc~.,
data= train_data,
kernel="linear",
cost= exp(seq(-5, 1, len = 50)))
summary(tune.out)
svmfit1 = tune.out$best.model
summary(svmfit1)
# Chunk 1
library(tidyverse)
library(broom)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(gbm)      # boosting technique
library(glmnet)
require(ggplot2)
# Chunk 2
## 250 obs. train data
train_data <- read.delim("./synth.tr", header = T, sep = "")
## 1000 obs. test data
test_data <- read.delim("./synth.te", header = T, sep = "")
# Chunk 3
train_data = train_data %>%
mutate(yc= as.factor(yc))
test_data = test_data %>%
mutate(yc= as.factor(yc))
# Chunk 4
## Using scaled data
set.seed (1)
tune.out = tune(svm, yc~.,
data= train_data,
kernel="linear",
cost= exp(seq(-5, 1, len = 50)))
summary(tune.out)
svmfit1 = tune.out$best.model
summary(svmfit1)
ypred1 = predict(svmfit1,test_data, type = "response") %>%
as.numeric()
ypred1
ypred1 = predict(svmfit1,test_data, type = "response")
test.error1 <- 1- sum(ypred1 == test_data$yc)/nrow(test_data)
test.error1
std.error1 <- sd((ypred1 - test_data$yc)^2)/sqrt(nrow(test_data))
std.error1
# Chunk 1
library(tidyverse)
library(broom)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(gbm)      # boosting technique
library(glmnet)
require(ggplot2)
# Chunk 2
## 250 obs. train data
train_data <- read.delim("./synth.tr", header = T, sep = "")
## 1000 obs. test data
test_data <- read.delim("./synth.te", header = T, sep = "")
# Chunk 3
train_data = train_data %>%
mutate(yc= as.factor(yc))
test_data = test_data %>%
mutate(yc= as.factor(yc))
# Chunk 4
## Using scaled data
set.seed (1)
tune.out = tune(svm, yc~.,
data= train_data,
kernel="linear",
cost= exp(seq(-5, 1, len = 50)))
summary(tune.out)
svmfit1 = tune.out$best.model
summary(svmfit1)
# Chunk 5
ypred1 = predict(svmfit1,test_data, type = "response")
test.error1 <- 1- sum(ypred1 == test_data$yc)/nrow(test_data)
test.error1
std.error1 <- sqrt((test.error * (1- test.error1))/(nrow(test_data)))
std.error1
ypred1 = predict(svmfit1,test_data, type = "response")
test.error1 <- 1- sum(ypred1 == test_data$yc)/nrow(test_data)
test.error1
std.error1 <- sqrt((test.error1 * (1- test.error1))/(nrow(test_data)))
std.error1
plot(svmfit1, train_data)
svmfit1$cost
plot(svmfit2, train_data)
set.seed (1)
tune.out2 = tune(svm, ys~., data=train_data, kernel="radial",
ranges= list(cost=c(0.1,1,10,100,1000),
gamma= c(0.5,1,2,3,4)))
summary(tune.out2)
## use the best-fit parameter to fit the model
svmfit2 = svm(yc~., data = train_data,
kernel="radial", gamma=0.5,cost =1)
summary(svmfit2)
plot(svmfit2, train_data)
## Using scaled data
set.seed (1)
tune.out = tune(svm, yc~.,
data= train_data,
kernel="linear",
ranges= list(cost=c(0.1,1,10,100,1000),
gamma= c(0.5,1,2,3,4)))
summary(tune.out)
svmfit1 = tune.out$best.model
summary(svmfit1)
ypred1 = predict(svmfit1,test_data, type = "response")
test.error1 <- 1- sum(ypred1 == test_data$yc)/nrow(test_data)
test.error1
std.error1 <- sqrt((test.error1 * (1- test.error1))/(nrow(test_data)))
std.error1
summary(svmfit1)
svmfit1$cost
## use the best-fit parameter to fit the model
svmfit1_model = svm(yc~., data = train_data,
kernel="radial", gamma=0.5,cost = 1)
summary(svmfit1_model)
plot(svmfit1_model, train_data)
# Chunk 1
library(tidyverse)
library(broom)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(gbm)      # boosting technique
library(glmnet)
require(ggplot2)
# Chunk 2
## 250 obs. train data
train_data <- read.delim("./synth.tr", header = T, sep = "")
## 1000 obs. test data
test_data <- read.delim("./synth.te", header = T, sep = "")
# Chunk 3
train_data = train_data %>%
mutate(yc= as.factor(yc))
test_data = test_data %>%
mutate(yc= as.factor(yc))
# Chunk 4
## Using scaled data
set.seed (1)
tune.out = tune(svm, yc~.,
data= train_data,
kernel="linear",
ranges= list(cost=c(0.1,1,10,100,1000),
gamma= c(0.5,1,2,3,4)))
summary(tune.out)
svmfit1 = tune.out$best.model
summary(svmfit1)
# Chunk 5
ypred1 = predict(svmfit1,test_data, type = "response")
test.error1 <- 1- sum(ypred1 == test_data$yc)/nrow(test_data)
test.error1
std.error1 <- sqrt((test.error1 * (1- test.error1))/(nrow(test_data)))
std.error1
# Chunk 6
## use the best-fit parameter to fit the model
svmfit1_model = svm(yc~., data = train_data,
kernel="radial", gamma=0.5,cost = 1)
summary(svmfit1_model)
plot(svmfit1_model, train_data)
# Chunk 7
set.seed (1)
tune.out2 = tune(svm, ys~., data=train_data,
kernel="radial",
ranges= list(cost=c(0.1,1,10,100,1000),
gamma= c(0.5,1,2,3,4)))
summary(tune.out2)
## use the best-fit parameter to fit the model
svmfit2 = svm(yc~., data = train_data,
kernel="radial", gamma=0.5,cost =1)
summary(svmfit2)
ypred2 = predict(svmfit2,
newdata = test_data, type = "response")
ypred2 = predict(svmfit2,test_data, type = "response")
test.error2 <- 1- sum(ypred2 == test_data$yc)/nrow(test_data)
test.error2
std.error2 <- sqrt((test.error2 * (1- test.error2))/(nrow(test_data)))
std.error2
plot(svmfit2, train_data)
ypred3 = predict.gbm(fit.adaboost,
newdata = test_data,
n.trees=50)
test.error3 <- 1- sum(ypred3 == test_data$yc)/nrow(test_data)
test.error3
std.error3 <- sqrt((test.error3 * (1- test.error3))/(nrow(test_data)))
std.error3
# Chunk 1
library(tidyverse)
library(broom)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(gbm)      # boosting technique
library(glmnet)
require(ggplot2)
# Chunk 2
## 250 obs. train data
train_data <- read.delim("./synth.tr", header = T, sep = "")
## 1000 obs. test data
test_data <- read.delim("./synth.te", header = T, sep = "")
# Chunk 3
train_data = train_data %>%
mutate(yc= as.factor(yc))
test_data = test_data %>%
mutate(yc= as.factor(yc))
# Chunk 4
## Using scaled data
set.seed (1)
tune.out = tune(svm, yc~.,
data= train_data,
kernel="linear",
ranges= list(cost=c(0.1,1,10,100,1000),
gamma= c(0.5,1,2,3,4)))
summary(tune.out)
svmfit1 = tune.out$best.model
summary(svmfit1)
# Chunk 5
ypred1 = predict(svmfit1,test_data, type = "response")
test.error1 <- 1- sum(ypred1 == test_data$yc)/nrow(test_data)
test.error1
std.error1 <- sqrt((test.error1 * (1- test.error1))/(nrow(test_data)))
std.error1
# Chunk 6
## use the best-fit parameter to fit the model
svmfit1_model = svm(yc~., data = train_data,
kernel="radial", gamma=0.5,cost = 1)
summary(svmfit1_model)
plot(svmfit1_model, train_data)
# Chunk 7
set.seed (1)
tune.out2 = tune(svm, ys~., data=train_data,
kernel="radial",
ranges= list(cost=c(0.1,1,10,100,1000),
gamma= c(0.5,1,2,3,4)))
summary(tune.out2)
# Chunk 8
## use the best-fit parameter to fit the model
svmfit2 = svm(yc~., data = train_data,
kernel="radial", gamma=0.5,cost =1)
summary(svmfit2)
# Chunk 9
ypred2 = predict(svmfit2,
newdata = test_data, type = "response")
ypred2 = predict(svmfit2,test_data, type = "response")
test.error2 <- 1- sum(ypred2 == test_data$yc)/nrow(test_data)
test.error2
std.error2 <- sqrt((test.error2 * (1- test.error2))/(nrow(test_data)))
std.error2
# Chunk 10
plot(svmfit2, train_data)
# Chunk 11
#install.packages("gbm")
library(gbm)
# Chunk 12
## use of gradient/generalized boosting models in "gbm".
set.seed(1118)
fit.adaboost <- gbm(yc ~., data=train_data,
distribution = "gaussian",
n.trees=50, ## No. of iterations = 50
shrinkage=0.01)
summary(fit.adaboost)
ypred3 = predict.gbm(fit.adaboost,
newdata = test_data,
n.trees=50)
test.error3 <- 1- sum(ypred3 == test_data$yc)/nrow(test_data)
test.error3
std.error3 <- sqrt((test.error3 * (1- test.error3))/(nrow(test_data)))
std.error3
ypred3 = predict(fit.adaboost,
newdata = test_data,
n.trees=50)
test.error3 <- 1- sum(ypred3 == test_data$yc)/nrow(test_data)
test.error3
std.error3 <- sqrt((test.error3 * (1- test.error3))/(nrow(test_data)))
std.error3
fit.adaboost <- gbm(yc ~., data=train_data,
n.trees=50, ## No. of iterations = 50
shrinkage=0.01)
summary(fit.adaboost)
library(ada)
#install.packages("gbm")
library(ada)
## use of gradient/generalized boosting models in "gbm".
set.seed(1118)
fit.adaboost <- ada(yc ~., data=train_data,
distribution = "gaussian",
n.trees=50, ## No. of iterations = 50
shrinkage=0.01)
summary(fit.adaboost)
fit.adaboost <- ada(yc ~., data=train_data,
n.trees=50, ## No. of iterations = 50
shrinkage=0.01)
summary(fit.adaboost)
fit.adaboost <- ada(yc ~., data=train_data,
iter=50, nu=0.1, bag.frac=0.5)
summary(fit.adaboost)
ypred3 = predict.gbm(fit.adaboost,
newdata = test_data,
n.trees=50)
test.error3 <- 1- sum(ypred3 == test_data$yc)/nrow(test_data)
test.error3
std.error3 <- sqrt((test.error3 * (1- test.error3))/(nrow(test_data)))
std.error3
ypred3 = predict.gbm(fit.adaboost,
newdata = test_data
)
test.error3 <- 1- sum(ypred3 == test_data$yc)/nrow(test_data)
test.error3
std.error3 <- sqrt((test.error3 * (1- test.error3))/(nrow(test_data)))
std.error3
ypred3 = predict.gbm(fit.adaboost,
newdata = test_data
)
test.error3 <- 1- sum(ypred3 == test_data$yc)/nrow(test_data)
test.error3
std.error3 <- sqrt((test.error3 * (1- test.error3))/(nrow(test_data)))
std.error3
ypred3 = predict(fit.adaboost,newdata = test_data)
test.error3 <- 1- sum(ypred3 == test_data$yc)/nrow(test_data)
test.error3
std.error3 <- sqrt((test.error3 * (1- test.error3))/(nrow(test_data)))
std.error3
test.error = c(test.error1, test.error2, test.error3)
sd = c(std.error1, std.error2, std.error3)
method = c("linear support vector classifier",
"Radial Kernel support vector classifier",
"Adaboost alogorithm")
summary = data.frame(method,test.error, sd )
knitr::kable(summary)
## Plotting for adaboost
plot2 = function(model,df_train,title){
train = df_train[,1:2]
L=75
X=seq(min(train[,1]),max(train[,1]),length=L)
Y=seq(min(train[,2]),max(train[,2]),length=L)
XY=expand.grid(X,Y) %>% rename(xs=Var1,ys=Var2)
yTrain = df_train$yc
yhat=predict(model,XY)
colors <- c("SkyBlue", "Orange")
yhat1 <- colors[as.numeric(yhat)]
yTrain <- colors[as.numeric(yTrain)]
plot(train, xlab="X1", ylab="X2", xlim=range(train[,1]), ylim=range(train[,2]), type="n")
points(XY,col=yhat1, pch=15,cex=0.1)
contour(X, Y, matrix(as.numeric(yhat),L,L), levels=c(1,2), add=TRUE, drawlabels=FALSE)
points(train, col=yTrain)
title(title)
}
plot2(fit.adaboost, test_data, "Adaboost contour graph")
plot(svmfit2, train_data, title("Radial Kernel support vector classifier "))
